# 转录转录
将 B 站长视频音频转录为文本，自用为主

# 工作栈

- [Yutto](https://github.com/yutto-dev/yutto)：下载音频
- ffmpeg：音频转码
- [FunASR](https://github.com/modelscope/FunASR)：ASR

# 环境配置
本仓库用 `uv` 管理依赖。~~不用 `uv` 的，我想你们肯定知道怎么做。~~

## 仅使用
```sh
git clone https://github.com/alephpi/TransTrans.git
cd TransTrans
uv venv
uv sync --production
```

## 开发
```sh
git clone https://github.com/alephpi/TransTrans.git
cd TransTrans
uv venv
uv sync
```

# 使用方式

## 示例
以 BV1iddQYQE7D 为例，运行
```sh
source .venv/bin/activate
python main.py BV1iddQYQE7D -w hotwords.txt
```
或者
```sh
uv python main.py BV1iddQYQE7D -w hotwords.txt
```

这将首先调用 `yutto` 下载音频，然后经过 `ffmpeg` 转码，然后用 `FunASR` 转录得到文本（有带时间戳字幕和纯文本两种），期间处理结果均保存于 `data/{bvid}` 文件夹下。

## 自定义热词
可在 `hotwords.txt` 中逐行添加音频中出现的高频词汇，以提高识别准确率。

# 配置要求
- 系统自行安装 ffmpeg（windows 注意配置其安装路径到环境变量）
- GPU 显存不少于 4G，如果运行时超出，适当缩小 `batch_size_s` 即可，`batch_size_s=300`时，显存占用约 2G。

# 笔记

## 选型
转录：根据 [论文](https://arxiv.org/pdf/2407.04051) 表格 6，Paraformer-zh 在 CER 和 RTF 上都达到最好。故不考虑 SenseVoice-small。

清理口语化表达：HanLP。

## 转录（`transcript.py`）
无论原音频编码如何，在 FunASR 中都以`torchaudio`导入并重采样至 16khz 处理。

仍以 BV1iddQYQE7D 为例（时长两小时），yutto 提供三种码率的`m4a`音频，编码均为 `aac`，其属性值如下（`torchaudio.info`）

|  码率   | 大小 | 采样率 | 帧数 |
| :-----: | :--: | :----: | :--: |
| 64kbps  | 35M  | 48khz  | 180k |
| 128kbps | 77M  | 48khz  | 360k |
| 320kbps | 138M | 48khz  | 360k |

对这些不同码率的音频，无论是 `torchaudio` 内部的重采样还是用 `ffmpeg` 的重采样，都得到 235M 的 `wav` 文件，且对识别结果无显著影响。

平均转录时长/音频时长比例（rtf_avg）为 0.008，即两小时音频一分钟转录完毕。

我们先不引入标点预测和句级别时间戳，而先只用字符级别时间戳，方便更精细的清理口语化表达。

## 清理口语化表达（`deoral.py`）

口语化特征：
1. 填充词 filler：就是、什么的、他妈的
2. 语气词：哎呀
3. 重复、修正 repetition
4. 成分省略

其中，1、2 可以通过简单正则匹配替换去除，而 3 不得不通过 POS tagging 来做。当然 1、2 也可以用 POS tagging 来做，只不过这样会加剧计算负担。

如果只是考虑总结文本大义，那么只需要把转录文本全部扔到 LLM 去做，但是 LLM 的生成性无法忠实还原言语顺序，因此我们择中选取这条较为繁琐的流程。

清理重复成分的策略很简单，即保留重复成分的最后一次出现，因为它相当于能指链滑行的终点、意义的锚定点。

## 设计
1. 使用时间戳信息我们可以找到口语停顿处，借此我们划分出**音句**。
2. 使用句子成分分析我们可以意义停顿处，借此划分出**意句**。

## 问题：
1. 词级别去重：先分句再去重还是先去重再分句？如果我们采用滑动窗口的办法去除重复就难以设定窗长。如果我们先分句，那么句子边界是自然的去重窗口。那么跨句的去重怎么办？
    - 词级别的去重不需要分词也可以做，但是会误伤某些叠词，例如“虎视眈眈”会被改成“虎视眈”。因此分词的目的在于把叠词词组保护起来，不被视为两个词。
2. 分词模型无论采用粗标准还是细标准都过于细碎，比如把“操你妈”拆分成三个词，而不是作为一个整体的詈语进行标记。
   - 把这些词加入自定义词组？
   - 干脆在分词前就清洗掉它们？
   - 但是反过来想，切的细碎反而有好处，例如“曹操他妈的完蛋了”就不会匹配到“操他妈的”，而是“他妈的”，因为曹操被分词保护起来。也就是说我们仍然可以在分词的基础上清洗，只不过我们的匹配词逻辑一定是从某个词开始，例如上面这个例子匹配只能从“曹操”或者“他”开始，规避了从“操”开始从而误伤曹操。
3. 句子级别的重复，考虑使用句子相似度模型？
4. 先考虑怎么去重，最后再考虑怎么反查删去的索引
5. 英文会破坏索引，因为对于 ASR 模型而言，每个英文词是一个字符，而对于分词模型而言，一个英文字符才是。为方便起见我们先假设文本中不含英文。

6. 我们先把填充词都删除，这样就可以进一步划分音句（因为填充词可以视为停顿），然后非常简便的把过短的音句删除，填充词不需要过度匹配，例如不需要匹配“他妈的”只需要匹配“他妈”，这样句首的“的”就可以删掉（虚词除语气词、关联词外无法作句首）。填充词包括：
   1. 脏话
   2. 语气词：哎呀、啊
   3. 设问、反问
   4. 连词：就是、就是说（但要区分强调的就是和连词性的就是，后者往往反复连续出现）
   5. 连续的重复：即用词的犹豫
   6. 不连续的（广义的）重复：
      1. 人称的重复，为了陈述观点（我）、询唤听众（你）、提起注意（它、这个），表现为说了一半从头开始说，通常是从某个人称开始说，例如代词+插入语+相同代词：我觉得我，你是你，我这个我，你这个你。这里的插入语也是常见词汇，但是必须是与上下文组合在一起才能判定为重复。
      2. 谓语的重复：就是、是、它是、就是说、（简单来）讲、也就是说：这里的特点是谓语动词常为“说、讲、是”
      3. 核心词的重复：近义词，这种重复是有益的，可以保留，反映出斟酌的即时状态。

## 模型的一些注意事项 

### FunASR
#### 语音识别模型
`paraformer-zh` 在 `punc_model` 空置，`sentence_timestamp=True` 时报错，这是因为模型需要标点来断句。
#### 标点预测模型
~~`ct-punc` 单独使用（而非接在 `paraformer-zh` 的流程中使用）的话，对英文的分词会有问题，可能与默认的 jieba 分词有关。~~ 应使用 `ct-punc-c` 而非 `ct-punc`，`ct-punc-c` 默认跳过英文部分（将其视为整体），而且不进行额外分词（汉字单字、连续英文字符算一词），速度更快。

### HanLP
#### 分词模型（TOK）
`COARSE_ELECTRA_SMALL_ZH`，`FINE_ELECTRA_SMALL_ZH`训练自 9970 万字的全世界最大中文分词（chinese word segmentation,CWS）语料库，F1 > 98%。

不加标点的分词速度：
加标点的分词速度：
#### 句子成分模型（POS）
`PKU_POS_ELECTRA_SMALL`：准确率 97.55%
`CTB9_POS_ELECTRA_SMALL`：准确率 96.26%
`C863_POS_ELECTRA_SMALL`：准确率 95.19%
我们选择 PKU 标准。

# Experiments

# TODO
- [x] 文本转录
- [ ] 清理口语化表达
- [ ] 自动切片
- [ ] 语音增强